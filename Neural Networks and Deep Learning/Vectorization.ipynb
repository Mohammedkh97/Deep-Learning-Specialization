{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vectorized result: 333332.69487703557\n",
      "Vectorized result: 333332.6948770214\n",
      "Non-vectorized time: 151.25203132629395 ms\n",
      "Vectorized time: 5.528926849365234 ms\n"
     ]
    }
   ],
   "source": [
    "# Vectorized and Non-Vectorized Examples\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Non-vectorized example: sum of squares\n",
    "def sum_of_squares_non_vectorized(arr):\n",
    "    total = 0\n",
    "    for x in arr:\n",
    "        total += x ** 2\n",
    "    return total\n",
    "\n",
    "# Vectorized example: sum of squares\n",
    "def sum_of_squares_vectorized(arr):\n",
    "    return np.sum(arr ** 2)\n",
    "arr = np.random.rand(1000000)\n",
    "# Timing non-vectorized\n",
    "start_non_vec = time.time()\n",
    "non_vec_result = sum_of_squares_non_vectorized(arr)\n",
    "end_non_vec = time.time()\n",
    "non_vec_time = end_non_vec - start_non_vec\n",
    "\n",
    "# Timing vectorized\n",
    "start_vec = time.time()\n",
    "vec_result = sum_of_squares_vectorized(arr)\n",
    "end_vec = time.time()\n",
    "vec_time = end_vec - start_vec\n",
    "\n",
    "print(\"Non-vectorized result:\", non_vec_result)\n",
    "print(\"Vectorized result:\", vec_result)\n",
    "print(\"Non-vectorized time:\", str(non_vec_time*1000) + \" ms\")\n",
    "print(\"Vectorized time:\", str(vec_time*1000) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e756bb",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91597a",
   "metadata": {},
   "source": [
    "Here‚Äôs the **vectorized forward pass** of a logistic regression model:\n",
    "\n",
    "### ‚úÖ Results (Predicted probabilities):\n",
    "\n",
    "```python\n",
    "[[0.5523, 0.5474, 0.5424, 0.5374, 0.5325]]\n",
    "```\n",
    "\n",
    "Each value is the predicted probability (between 0 and 1) for one of the 5 examples.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç How It Works\n",
    "\n",
    "#### Inputs:\n",
    "\n",
    "* `X`: shape **(3, 5)** ‚Äî 3 features, 5 training examples\n",
    "* `W`: shape **(1, 3)** ‚Äî weight vector\n",
    "* `b`: scalar bias\n",
    "\n",
    "#### Vectorized Computation:\n",
    "\n",
    "$$\n",
    "Z = W \\cdot X + b \\quad \\text{(shape: 1 √ó 5)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A = \\sigma(Z) = \\frac{1}{1 + e^{-Z}} \\quad \\text{(sigmoid applied element-wise)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Why It‚Äôs Efficient\n",
    "\n",
    "* No loops!\n",
    "* Processes all examples in one matrix multiplication\n",
    "* GPU/CPU optimized\n",
    "* Clear, concise, and scalable to millions of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb49476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55230791, 0.54735762, 0.54239794, 0.53742985, 0.53245431]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate data: 3 features, 5 examples\n",
    "X = np.array([[0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "              [1.0, 0.9, 0.8, 0.7, 0.6],\n",
    "              [0.5, 0.3, 0.1, -0.1, -0.3]])  # shape: (3, 5)\n",
    "\n",
    "# Simulated weights and bias\n",
    "W = np.array([[0.3, -0.2, 0.5]])  # shape: (1, 3)\n",
    "b = 0.1\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Vectorized Logistic Regression forward pass\n",
    "def logistic_regression_forward(X, W, b):\n",
    "    Z = np.dot(W, X) + b      # Linear part (1, 5)\n",
    "    A = sigmoid(Z)            # Activation (1, 5)\n",
    "    return A\n",
    "\n",
    "# Forward pass\n",
    "A = logistic_regression_forward(X, W, b)\n",
    "A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605b715",
   "metadata": {},
   "source": [
    "## Vectorized cost function, backward propagation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410196f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.679855\n",
      "dW:\n",
      "[[-0.03655168 -0.04509568 -0.00377565]]\n",
      "db: -0.057610\n"
     ]
    }
   ],
   "source": [
    "# Simulated labels for 5 examples (binary classification)\n",
    "Y = np.array([[1, 0, 1, 0, 1]])  # shape: (1, 5)\n",
    "\n",
    "# Cost function (cross-entropy loss) - vectorized\n",
    "def compute_cost(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = - (1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    return np.squeeze(cost)  # make sure cost is a scalar\n",
    "\n",
    "# Backward propagation (gradients) - vectorized\n",
    "def backward_propagation(X, A, Y):\n",
    "    m = X.shape[1]\n",
    "    dZ = A - Y                            # (1, m)\n",
    "    dW = (1 / m) * np.dot(dZ, X.T)       # (1, 3)\n",
    "    db = (1 / m) * np.sum(dZ)            # scalar\n",
    "    return dW, db\n",
    "\n",
    "# Compute cost and gradients\n",
    "cost = compute_cost(A, Y)\n",
    "dW, db = backward_propagation(X, A, Y)\n",
    "\n",
    "print(f\"Cost: {cost:.6f}\")\n",
    "print(f\"dW:\\n{dW}\")\n",
    "print(f\"db: {db:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2201bd",
   "metadata": {},
   "source": [
    "### üßÆ How It Works (Vectorized)\n",
    "\n",
    "#### Cost Function:\n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{m} \\sum \\left[ y \\log(a) + (1 - y)\\log(1 - a) \\right]\n",
    "$$\n",
    "\n",
    "#### Gradients:\n",
    "\n",
    "* $dZ = A - Y$\n",
    "* $dW = \\frac{1}{m} \\cdot dZ \\cdot X^T$\n",
    "* $db = \\frac{1}{m} \\cdot \\sum dZ$\n",
    "\n",
    "These are used during **gradient descent** to update parameters:\n",
    "\n",
    "$$\n",
    "W := W - \\alpha dW,\\quad b := b - \\alpha db\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Would you like:\n",
    "\n",
    "1. A complete training loop using these equations?\n",
    "2. A side-by-side comparison of vectorized vs. non-vectorized logistic regression?\n",
    "3. A downloadable Jupyter notebook version?\n",
    "\n",
    "Let me know how you'd like to continue!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53413179",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e5db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
